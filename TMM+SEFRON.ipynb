{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "oAeC2iI3VDzO"
      },
      "outputs": [],
      "source": [
        "from itertools import count\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy as sp\n",
        "import scipy.sparse as sparse\n",
        "import torch, torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def pop_code(sample, cen, wid, P,gamma,T,T_enc):\n",
        "    P = cen.shape[0]\n",
        "   # n_feature = sample.shape[1]\n",
        "    x_sample = sp.tile(sample, (P, 1))#The numpy.tile() function constructs a new array by repeating array â€“ 'arr', the number of times we want to repeat as per repetitions.\n",
        "    x_cen = sp.tile(cen, (1, n_feature))\n",
        "\n",
        "    grf_act = sp.exp(- (x_sample - x_cen)**2 / (2 * (wid**2)))  # activation of the receptive fields\n",
        "\n",
        "    spike_times = ((1 - grf_act.flatten('F')) * T_enc) #rounding off\n",
        "\n",
        "    # ignore boundary spikes (spikes at params[T_pop]). These spikes are only occurring because of rounding.\n",
        "    actual_spikes = grf_act.flatten('F') != 0\n",
        "    n_actual_spikes = sp.sum(actual_spikes)\n",
        "    #print(\"sp.arange\",(sp.arange(n_feature*P)[actual_spikes],\n",
        "    #spike_times[actual_spikes]))\n",
        "    #print(\"spike_times\",spike_times)\n",
        "    #print(\"sp.ones\",(sp.ones(n_actual_spikes)))\n",
        "    #print( \"last\",(n_feature*P, T),dtype=int)\n",
        "    return sparse.csr_matrix((sp.ones(n_actual_spikes), (sp.arange(n_feature*P)[actual_spikes],\n",
        "                                            spike_times[actual_spikes])), shape=(n_feature*P, T), dtype=int)"
      ],
      "metadata": {
        "id": "eAkB7atfVnIx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hid_out(spike_pattern,w,hid_thres,norm_psp):\n",
        "  n_output=w.shape[1]\n",
        "  (in_spike, in_feature) = spike_pattern.T.nonzero()\n",
        "  #delay_by_tau = np.arange(T, dtype=float) / 300\n",
        "  #norm_psp = np.triu(lg.toeplitz(delay_by_tau * np.exp(1 - delay_by_tau)))\n",
        "  T=norm_psp.shape[1]\n",
        "  #print('hi1')\n",
        "  out_spike_pattern = sparse.lil_matrix((n_output, T))\n",
        "  last_out_spike = -1 * sp.ones((1, n_output))\n",
        "  while True:\n",
        "      psp = sp.dot((w[in_feature, :] * (in_spike[:, sp.newaxis] > last_out_spike)).T, norm_psp[in_spike, :])\n",
        "      #print(psp.shape)\n",
        "      #print(psp>hid_thres[:,sp.newaxis])\n",
        "      out_spike = sp.argmax(psp > hid_thres[:, sp.newaxis], axis=1)\n",
        "      #print(out_spike)\n",
        "      act_spike = psp[sp.arange(n_output), out_spike] > hid_thres\n",
        "      #print(act_spike)\n",
        "      if sp.sum(act_spike) == 0:\n",
        "          break\n",
        "\n",
        "      out_spike_pattern[sp.arange(n_output)[act_spike], out_spike[act_spike]] = 1\n",
        "      last_out_spike[0, act_spike] = out_spike[act_spike]\n",
        "      #print(out_spike_pattern.tocsr())\n",
        "\n",
        "  return out_spike_pattern.tocsr()"
      ],
      "metadata": {
        "id": "j_4N_FyiZ9MQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def spike_response(s, tau):\n",
        "    x = s/tau\n",
        "    LIF_norm = x * np.exp(1-x)\n",
        "    LIF_norm[LIF_norm<0]=0\n",
        "\n",
        "    return LIF_norm"
      ],
      "metadata": {
        "id": "GBjZl25N3wSB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sefron_out(spike,w,thres,norm_psp,n_neuron,n_class,tau):\n",
        "\n",
        "  W_sample = weight_syn[np.tile(np.arange(n_neuron), (1, n_class)),\n",
        "                             np.array(np.tile(spiked_hid_neurons, (1, n_class)), dtype=np.int32) - 1,\n",
        "                             np.ravel(np.tile(np.arange(n_class), (n_neuron, 1)), order='F')]\n",
        "  wh = np.reshape(W_sample, (n_neuron, n_class), order='F')\n",
        "  t = np.matmul(np.ones((n_neuron, 1)), np.arange(T+1).reshape(1, int(T+1)))\n",
        "\n",
        "\n",
        "  V = np.matmul(wh.transpose(), spike_response(t - spike[:, np.newaxis], tau))\n",
        "  firing = (V>thres[:, np.newaxis])\n",
        "  firing_time = np.argmax(firing.transpose(), axis=0)\n",
        "  firing_time[firing_time==0] = T - 1\n",
        "\n",
        "  tc = firing_time\n",
        "\n",
        "      #print(out_spike_pattern.tocsr())\n",
        "\n",
        "  return tc,V"
      ],
      "metadata": {
        "id": "22eRx3rlWwql"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/ae_train_1.csv',header=None) #fix header issue\n",
        "train_sh= train.sample(frac=1)\n",
        "test=pd.read_csv('/content/ae_test_1.csv',header=None)\n",
        "test_sh= test.sample(frac=1)\n",
        "#test.sample(frac=1) is done to ensure the first row is not considered as header"
      ],
      "metadata": {
        "id": "CFistyStVuUZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def STDP_norm(r_time, tau):\n",
        "    stdp = np.exp(-np.abs(r_time)/tau)\n",
        "\n",
        "    g = deepcopy(r_time)\n",
        "    g[g>0] = 1\n",
        "    g[g<0] = 0\n",
        "    g = g*stdp\n",
        "\n",
        "    if (isinstance(np.sum(g, axis=0), np.ndarray)):\n",
        "        pos_weight = g/((np.sum(g, axis=0))[np.newaxis, :])\n",
        "    else:\n",
        "        pos_weight = g/np.sum(g)\n",
        "\n",
        "    temp = np.isnan(pos_weight)\n",
        "    pos_weight[temp]=0\n",
        "\n",
        "    g= deepcopy(r_time)\n",
        "    g[g>0] = 0\n",
        "    g[g<0] = 1\n",
        "    g = g * stdp\n",
        "\n",
        "    if (isinstance(np.sum(g, axis=0), np.ndarray)):\n",
        "        neg_weight = -1 * (g/((np.sum(g, axis=0))[np.newaxis, :]))\n",
        "    else:\n",
        "        neg_weight = -1 * (g/np.sum(g))\n",
        "\n",
        "    temp = np.isnan(neg_weight)\n",
        "    neg_weight[temp] = 0\n",
        "\n",
        "    Ut = pos_weight + neg_weight\n",
        "\n",
        "    return Ut"
      ],
      "metadata": {
        "id": "oSDVEtJa4ec3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_function( A, B):\n",
        "  result = []\n",
        "  sigma=.55/0.01\n",
        "  for i in range(A.shape[0]):\n",
        "    result.append(np.exp((-np.power(A[i]-B, 2)) / (2*(sigma**2))))\n",
        "  return np.array(result)"
      ],
      "metadata": {
        "id": "iH-N5Xn-_wCW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minimum(a, b):\n",
        "\n",
        "    if a <= b:\n",
        "        return a\n",
        "    else:\n",
        "        return b"
      ],
      "metadata": {
        "id": "nY1QdkVv8LBd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.linalg as lg\n",
        "import sys\n",
        "P=6\n",
        "gamma=.7\n",
        "T=600\n",
        "T_b=300\n",
        "T_enc = 300\n",
        "cen = (2 * (np.arange(1,P+1, dtype=float) )- 3) / (2 * (P - 2))\n",
        "tau=T/2\n",
        "marg=-(sys.maxsize)- 1\n",
        "wid = 1 / (gamma * (P - 2))\n",
        "n_sample, n_feature =train_sh.shape\n",
        "n_feature -= 1\n",
        "n_class = np.size(np.unique(train_sh.iloc[:, -1]))\n",
        "class_of_neuron=np.zeros(1500)\n",
        "n_neuron=0\n",
        "n_neurons_in_class=np.zeros(n_class)\n",
        "hidden_list=[]\n",
        "U_TID_list=[]\n",
        "psp_2_list=[]\n",
        "l_rate=0.5\n",
        "theta_sefron=np.zeros(n_class)\n",
        "delay_by_tau = np.arange(T, dtype=float) / 300\n",
        "norm_psp = np.triu(lg.toeplitz(delay_by_tau * np.exp(1 - delay_by_tau)))\n",
        "V_i=np.zeros((n_sample,n_feature*P),dtype=int)\n",
        "hid_thres=np.zeros(1500)\n",
        "T_input=T/3\n",
        "alpha=.60\n",
        "V_k=np.empty((n_sample,1500))\n",
        "Aplus=1\n",
        "weight_ik=np.empty((n_feature*P,1500))\n",
        "out_size=0\n",
        "\n",
        "no_epochs=1 #no of epochs\n",
        "test_acc_hid=[]\n",
        "train_acc_hid=[]\n",
        "seen=[]\n",
        "Tm_sef=0.05/.01\n",
        "for e in range(no_epochs):\n",
        "  count_acc=0\n",
        "  CC_n_t=[]\n",
        "  MC_n_t=[]\n",
        "  count_tacc=0\n",
        "  t_cc=-1\n",
        "  t_mc=-1\n",
        "  t_train=np.arange(T+1)\n",
        "  #change the order of the input sample\n",
        "  #cnf_mat = np.zeros((3, 3))\n",
        "\n",
        "  for s in range(n_sample):\n",
        "    CC_n=[]\n",
        "    MC_n=[]\n",
        "\n",
        "    curr_class=train_sh.iloc[s,-1]\n",
        "\n",
        "    sample=train_sh.iloc[s,:-1]\n",
        "    sample = sample[sp.newaxis, :]\n",
        "\n",
        "\n",
        "    spike_pattern = pop_code(sample, cen[:, sp.newaxis], wid, P,gamma,T,T_enc)\n",
        "\n",
        "\n",
        "    if ((n_neurons_in_class[curr_class-1]==0)and (e==0)):\n",
        "\n",
        "      n_neurons_in_class[curr_class-1]+=1\n",
        "      (neuron,spike_time)=spike_pattern.nonzero()\n",
        "      class_of_neuron[n_neuron]=curr_class\n",
        "      psp4spike = norm_psp[spike_time, 200]\n",
        "\n",
        "      total_pre_psp = np.zeros(n_feature*P)\n",
        "      np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "      weight_ik[:, n_neuron] = total_pre_psp / np.sum(total_pre_psp)\n",
        "      hid_thres[n_neuron] = np.dot(weight_ik[:, n_neuron].T, total_pre_psp)\n",
        "\n",
        "      n_neuron += 1\n",
        "      weight_syn=np.zeros((int(n_neuron),int(T+1),int(n_class)))\n",
        "    else:\n",
        "      #computing hidden neuron output.\n",
        "\n",
        "      #weight_syn.shape=(int(n_neuron),int(T+1),int(n_class))\n",
        "      hidden=hid_out(spike_pattern,weight_ik[:,:n_neuron],hid_thres[:n_neuron],norm_psp)\n",
        "\n",
        "      ## We know what time the neurons have spiked using the hidden variable above\n",
        "\n",
        "\n",
        "\n",
        "      #index of hidden neurons that have spiked\n",
        "      spiked_hid_neurons=np.argmax(hidden.toarray(),axis=1)\n",
        "      hidden_list.append(spiked_hid_neurons)\n",
        "\n",
        "      U_TID=STDP_norm((300-spiked_hid_neurons),tau)\n",
        "      psp_2=norm_psp[spiked_hid_neurons,300]\n",
        "      U_TID_list.append(U_TID)\n",
        "      psp_2_list.append(psp_2)\n",
        "      #print(spiked_hid_neurons)\n",
        "      #print(spiked_hid_neurons.shape,U_TID.shape,psp_2.shape)\n",
        "      if(theta_sefron[curr_class-1]==0):\n",
        "        weight_syn[:,:,curr_class-1]=np.multiply((gaussian_function(t_train, spiked_hid_neurons.transpose())).transpose(), U_TID[:, np.newaxis])\n",
        "\n",
        "        theta_sefron[curr_class-1] = (np.matmul(U_TID[np.newaxis, :], psp_2[:, np.newaxis])).squeeze()\n",
        "      for i in range(n_neuron):\n",
        "        if(class_of_neuron[i]==curr_class):\n",
        "\n",
        "          CC_n.append(i)\n",
        "\n",
        "      min=T+1\n",
        "      CC=-1\n",
        "      for i in CC_n:\n",
        "        if(spiked_hid_neurons[i]<min):\n",
        "          min=spiked_hid_neurons[i]\n",
        "          CC=i\n",
        "      t_cc=min\n",
        "      CC=i\n",
        "\n",
        "\n",
        "      for i in range(n_neuron):\n",
        "        if(class_of_neuron[i]!=curr_class):\n",
        "\n",
        "          MC_n.append(i)\n",
        "      min_2=T+1\n",
        "      MC=-1\n",
        "\n",
        "      for i in MC_n:\n",
        "        if(i<n_neuron):\n",
        "          if(spiked_hid_neurons[i]<min_2):\n",
        "            min_2=spiked_hid_neurons[i]\n",
        "            MC=i\n",
        "      t_mc=min_2\n",
        "      MC=i\n",
        "\n",
        "      #margin calculation\n",
        "\n",
        "      if((t_mc!=-1) and(t_cc!=-1)):\n",
        "        marg=t_mc-t_cc\n",
        "      T_m=28\n",
        "      T_a=alpha*T\n",
        "      eta=0.001\n",
        "      if (e==0):\n",
        "        if((CC==-1) or ((CC!=-1)and (t_cc>T_a))and (n_neuron < 1500)):\n",
        "          n_neurons_in_class[curr_class-1]+=1\n",
        "          class_of_neuron[n_neuron]=curr_class\n",
        "          spike_pattern = pop_code(sample, cen[:, sp.newaxis], wid, P,gamma,T,T_enc)\n",
        "          (neuron,spike_time)=spike_pattern.nonzero()\n",
        "\n",
        "          psp4spike = norm_psp[spike_time, 200]\n",
        "          total_pre_psp = np.zeros(n_feature * P)\n",
        "          np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "\n",
        "                  # initialize the weights and threshold\n",
        "          weight_ik[:, n_neuron] = total_pre_psp / np.sum(total_pre_psp)\n",
        "          hid_thres[n_neuron] = np.dot(weight_ik[:, n_neuron].T, total_pre_psp)\n",
        "          n_neuron+=1\n",
        "\n",
        "\n",
        "        elif((MC!=-1)and(marg<T_m)):\n",
        "          (neuron, spike_time) = spike_pattern.nonzero()\n",
        "\n",
        "\n",
        "      dummy,V=sefron_out(spiked_hid_neurons,weight_syn,theta_sefron,norm_psp,n_neuron,n_class,tau)\n",
        "\n",
        "      Other_class = np.array(list(set(np.arange(n_class)) - set([curr_class-1])))\n",
        "      tcc_sef=dummy[curr_class-1]\n",
        "      tmc_sef=np.amin(dummy[Other_class])\n",
        "      reference_time = deepcopy(dummy)\n",
        "\n",
        "      if(tmc_sef<tcc_sef+Tm_sef):\n",
        "        #300 is Td\n",
        "        if (tcc_sef > 300-1):\n",
        "                    reference_time[curr_class-1] = 300 - 1\n",
        "        trf_mc = minimum(T-1, tcc_sef+Tm_sef)\n",
        "        Wrng_class = np.where(dummy[Other_class] < tcc_sef+Tm_sef)[0]\n",
        "        reference_time[Other_class[Wrng_class]] = trf_mc\n",
        "        r_time = (dummy[np.newaxis, :]+1) - spiked_hid_neurons[:, np.newaxis]\n",
        "\n",
        "        Ut = STDP_norm(r_time, tau)\n",
        "\n",
        "        r_time = (reference_time[np.newaxis, :]+1) - spiked_hid_neurons[:, np.newaxis]\n",
        "\n",
        "        Ut_de = STDP_norm(r_time, tau)\n",
        "\n",
        "        w_tf = theta_sefron / np.sum(Ut * spike_response((dummy[np.newaxis, :]+1) - spiked_hid_neurons[:, np.newaxis], tau), axis=0)\n",
        "\n",
        "        w_td = theta_sefron/ np.sum(Ut_de * spike_response((reference_time[np.newaxis, :]+1) - spiked_hid_neurons[:, np.newaxis], tau), axis=0)\n",
        "\n",
        "        delta_W = np.multiply(Ut_de, (w_td - w_tf)[np.newaxis, :])\n",
        "\n",
        "        delta_wx = np.multiply(gaussian_function(np.arange(T+1), np.tile(spiked_hid_neurons, (1, n_class)).squeeze()).transpose(), np.ravel(delta_W, order='F')[:, np.newaxis])\n",
        "        weight_syn = weight_syn + l_rate * np.transpose(np.reshape(delta_wx.transpose(), (int(T+1), np.array(n_neuron, dtype=np.int32), n_class), order='F'), (1, 0, 2))\n",
        "        weight_syn[weight_syn==-np.inf] = np.inf\n",
        "        weight_syn[np.isnan(weight_syn)] = np.inf\n",
        "\n",
        "\n",
        "      print(spiked_hid_neurons,dummy,curr_class,tcc_sef,tmc_sef,weight_syn.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EbR0ZW-o0ob3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06e716d-5f10-4fae-c26a-277dedb61af5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[319 227 274] [599 599 301 599] 3 301 599 (3, 601, 4)\n",
            "[344 252 257] [599 599 305 599] 3 305 599 (3, 601, 4)\n",
            "[203 316 380] [599 599 599 301] 4 301 599 (3, 601, 4)\n",
            "[352 266 218] [301 599 320 599] 1 301 320 (3, 601, 4)\n",
            "[348 259 269] [331 599 315 599] 3 315 331 (3, 601, 4)\n",
            "[201 303 373] [535 599 599 299] 4 299 535 (3, 601, 4)\n",
            "[357 267 208] [298 599 331 599] 1 298 331 (3, 601, 4)\n",
            "[203 303 369] [536 599 599 301] 4 301 536 (3, 601, 4)\n",
            "[368 287 211 339] [599 599 599 599] 1 599 599 (4, 601, 4)\n",
            "[204 298 364 229] [599 599 599 599] 4 599 599 (4, 601, 4)\n",
            "[370 292 201 342] [599 599 599 599] 1 599 599 (4, 601, 4)\n",
            "[318 224 274 316] [599 599 599 599] 3 599 599 (4, 601, 4)\n",
            "[371 292 210 342] [329 599 599 599] 1 329 599 (4, 601, 4)\n",
            "[240 326 367 213] [599 301 599 599] 2 301 599 (4, 601, 4)\n",
            "[202 321 381 232] [599 308 599 599] 4 599 308 (4, 601, 4)\n",
            "[360 281 261 350] [599 599 599 599] 3 599 599 (4, 601, 4)\n",
            "[217 337 391 260] [599 599 599 599] 4 599 599 (4, 601, 4)\n",
            "[255 322 356 213] [599 316 599 599] 2 316 599 (4, 601, 4)\n",
            "[326 235 264 324] [599 599 313 599] 3 313 599 (4, 601, 4)\n",
            "[250 314 349 204] [599 310 599 599] 2 310 599 (4, 601, 4)\n",
            "[327 232 260 321] [599 599 310 599] 3 310 599 (4, 601, 4)\n",
            "[362 283 221 330] [328 599 330 599] 1 328 330 (4, 601, 4)\n",
            "[208 329 389 259] [599 599 599 347] 4 347 599 (4, 601, 4)\n",
            "[200 306 372 231] [462 599 599 325] 4 325 462 (4, 601, 4)\n",
            "[253 325 358 211] [584 314 599 599] 2 314 584 (4, 601, 4)\n",
            "[215 307 365 232] [470 599 599 332] 4 332 470 (4, 601, 4)\n",
            "[224 316 364 204] [466 297 599 352] 2 297 352 (4, 601, 4)\n",
            "[219 271 347 239] [444 599 487 331] 4 331 444 (4, 601, 4)\n",
            "[268 337 367 230] [599 599 599 599] 2 599 599 (4, 601, 4)\n",
            "[349 242 259 316] [599 599 313 599] 1 599 313 (4, 601, 4)\n",
            "[266 320 346 211] [525 294 599 599] 2 294 525 (4, 601, 4)\n",
            "[362 294 339 362] [599 599 599 599] 3 599 599 (4, 601, 4)\n",
            "[217 314 366 206] [438 268 331 340] 2 268 331 (4, 601, 4)\n",
            "[232 321 365 207] [473 273 340 364] 2 273 340 (4, 601, 4)\n",
            "[361 272 225 329] [293 599 283 599] 1 293 283 (4, 601, 4)\n",
            "[293 208 290 285] [599 570 251 599] 3 251 570 (4, 601, 4)\n",
            "[315 319 317 256] [599 599 387 599] 1 599 387 (4, 601, 4)\n",
            "[316 296 287 262] [599 599 320 599] 1 599 320 (4, 601, 4)\n",
            "[227 311 358 202] [364 270 333 599] 2 270 333 (4, 601, 4)\n",
            "[238 330 375 224] [406 283 364 359] 2 283 359 (4, 601, 4)\n",
            "[313 223 269 307] [290 599 253 599] 3 253 290 (4, 601, 4)\n",
            "[199 313 379 240] [337 292 338 329] 4 329 292 (4, 601, 4)\n",
            "[201 322 384 243] [349 293 350 353] 4 353 293 (4, 601, 4)\n",
            "[292 224 299 295] [599 599 253 599] 3 253 599 (4, 601, 4)\n",
            "[226 320 367 207] [376 272 345 599] 2 272 345 (4, 601, 4)\n",
            "[239 326 367 212] [409 279 357 599] 2 279 357 (4, 601, 4)\n",
            "[208 286 364 239] [318 317 308 338] 4 338 308 (4, 601, 4)\n",
            "[260 322 352 211] [485 294 367 599] 2 294 367 (4, 601, 4)\n",
            "[204 325 387 254] [370 314 362 319] 4 319 314 (4, 601, 4)\n",
            "[198 311 377 234] [340 300 338 295] 4 295 300 (4, 601, 4)\n",
            "[323 222 257 309] [282 599 253 599] 3 253 282 (4, 601, 4)\n",
            "[361 261 244 329] [283 599 278 599] 1 283 278 (4, 601, 4)\n",
            "[333 235 252 324] [279 599 261 599] 3 261 279 (4, 601, 4)\n",
            "[318 224 271 315] [291 599 256 599] 3 256 291 (4, 601, 4)\n",
            "[364 262 250 333] [286 599 281 599] 1 286 281 (4, 601, 4)\n",
            "[305 304 305 245] [599 599 385 599] 1 599 385 (4, 601, 4)\n",
            "[238 318 358 204] [381 281 349 320] 2 281 320 (4, 601, 4)\n",
            "[297 208 283 284] [599 599 263 599] 3 263 599 (4, 601, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdVdGGHH8PZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_neuron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VouzYe2yG7q",
        "outputId": "088950fe-43aa-4b73-a98c-451a314e9723"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(hidden_list)):\n",
        "  while len(hidden_list[i])<n_neuron:\n",
        "   hidden_list[i]=np.insert(hidden_list[i],len(hidden_list[i])-1,0)\n"
      ],
      "metadata": {
        "id": "9Dy3rXcCy3uz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_list"
      ],
      "metadata": {
        "id": "27AiZyxpu5ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c948592f-c69d-44fe-9823-45b9391cf6d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([  0,   0,   0, 217]),\n",
              " array([303,   0,   0, 278]),\n",
              " array([205,   0,   0, 259]),\n",
              " array([280,   0,   0, 207]),\n",
              " array([241,   0,   0, 296]),\n",
              " array([271,   0,   0, 229]),\n",
              " array([277, 260,   0, 359]),\n",
              " array([211, 244, 289, 287]),\n",
              " array([312, 348, 218, 203]),\n",
              " array([208, 266, 298, 283]),\n",
              " array([289, 330, 226, 204]),\n",
              " array([204, 270, 327, 322]),\n",
              " array([315, 351, 234, 207]),\n",
              " array([336, 336, 212, 262]),\n",
              " array([224, 286, 345, 345]),\n",
              " array([212, 239, 290, 293]),\n",
              " array([269, 323, 363, 360]),\n",
              " array([321, 350, 216, 210]),\n",
              " array([251, 225, 340, 356]),\n",
              " array([314, 350, 224, 203]),\n",
              " array([347, 337, 232, 289]),\n",
              " array([255, 218, 333, 351]),\n",
              " array([335, 326, 219, 271]),\n",
              " array([291, 266, 288, 317]),\n",
              " array([339, 346, 214, 258]),\n",
              " array([225, 233, 303, 306]),\n",
              " array([270, 236, 352, 367]),\n",
              " array([303, 339, 224, 205]),\n",
              " array([268, 314, 237, 213]),\n",
              " array([267, 218, 344, 360]),\n",
              " array([202, 262, 321, 313]),\n",
              " array([303, 337, 231, 221]),\n",
              " array([325, 333, 202, 244]),\n",
              " array([331, 334, 206, 254]),\n",
              " array([320, 293, 285, 317]),\n",
              " array([333, 323, 218, 271]),\n",
              " array([329, 336, 203, 246]),\n",
              " array([326, 359, 234, 211]),\n",
              " array([274, 242, 355, 370]),\n",
              " array([204, 244, 316, 319]),\n",
              " array([324, 356, 223, 209]),\n",
              " array([307, 342, 219, 203]),\n",
              " array([323, 317, 220, 266]),\n",
              " array([340, 365, 234, 236]),\n",
              " array([296, 334, 224, 206]),\n",
              " array([273, 209, 343, 360]),\n",
              " array([271, 243, 354, 370]),\n",
              " array([328, 326, 209, 257]),\n",
              " array([318, 328, 205, 242]),\n",
              " array([331, 362, 236, 216]),\n",
              " array([321, 324, 208, 252]),\n",
              " array([329, 315, 231, 280]),\n",
              " array([204, 259, 326, 323]),\n",
              " array([332, 319, 223, 276]),\n",
              " array([336, 336, 211, 262]),\n",
              " array([325, 352, 214, 215]),\n",
              " array([201, 257, 319, 313]),\n",
              " array([201, 259, 319, 312])]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_syn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9pbcYCMAt7B",
        "outputId": "1a626f46-11de-40e5-a5d2-e87090deda8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.00000000e+00,  2.36163232e-04,  0.00000000e+00,\n",
              "          5.29792976e-05],\n",
              "        [ 0.00000000e+00,  2.52931390e-04,  0.00000000e+00,\n",
              "          5.72685992e-05],\n",
              "        [ 0.00000000e+00,  2.70800593e-04,  0.00000000e+00,\n",
              "          6.18847093e-05],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  3.63501158e-12,  0.00000000e+00,\n",
              "          2.06685209e-10],\n",
              "        [ 0.00000000e+00,  3.19479009e-12,  0.00000000e+00,\n",
              "          1.83343638e-10],\n",
              "        [ 0.00000000e+00,  2.80695397e-12,  0.00000000e+00,\n",
              "          1.62584344e-10]],\n",
              "\n",
              "       [[ 0.00000000e+00, -2.56781240e-07,  0.00000000e+00,\n",
              "         -3.44628590e-08],\n",
              "        [ 0.00000000e+00, -2.83787158e-07,  0.00000000e+00,\n",
              "         -3.82513829e-08],\n",
              "        [ 0.00000000e+00, -3.13529651e-07,  0.00000000e+00,\n",
              "         -4.24423488e-08],\n",
              "        ...,\n",
              "        [ 0.00000000e+00, -5.66211858e-07,  0.00000000e+00,\n",
              "         -9.92814015e-07],\n",
              "        [ 0.00000000e+00, -5.13516605e-07,  0.00000000e+00,\n",
              "         -9.04294355e-07],\n",
              "        [ 0.00000000e+00, -4.65571572e-07,  0.00000000e+00,\n",
              "         -8.23394898e-07]],\n",
              "\n",
              "       [[ 0.00000000e+00,  8.20816414e-06,  0.00000000e+00,\n",
              "         -1.28884102e-08],\n",
              "        [ 0.00000000e+00,  8.92566593e-06,  0.00000000e+00,\n",
              "         -1.43478657e-08],\n",
              "        [ 0.00000000e+00,  9.70267881e-06,  0.00000000e+00,\n",
              "         -1.59673075e-08],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  1.12413831e-09,  0.00000000e+00,\n",
              "         -2.19989120e-06],\n",
              "        [ 0.00000000e+00,  1.00313741e-09,  0.00000000e+00,\n",
              "         -2.00971854e-06],\n",
              "        [ 0.00000000e+00,  8.94865029e-10,  0.00000000e+00,\n",
              "         -1.83537878e-06]],\n",
              "\n",
              "       [[ 0.00000000e+00,  1.04389344e-05,  0.00000000e+00,\n",
              "          5.20321391e-04],\n",
              "        [ 0.00000000e+00,  1.13401829e-05,  0.00000000e+00,\n",
              "          5.56345125e-04],\n",
              "        [ 0.00000000e+00,  1.23151691e-05,  0.00000000e+00,\n",
              "          5.94666295e-04],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  7.90074660e-10,  0.00000000e+00,\n",
              "          2.98054449e-12],\n",
              "        [ 0.00000000e+00,  7.04333112e-10,  0.00000000e+00,\n",
              "          2.61525654e-12],\n",
              "        [ 0.00000000e+00,  6.27688988e-10,  0.00000000e+00,\n",
              "          2.29397889e-12]]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theta_sefron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_f-rupbHXXe",
        "outputId": "5a0c6876-5f43-4f61-afad-ae0d0fa9c14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.2385555 , 0.4063158 , 0.29746976, 0.56530122])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is Additional Old code please ignore"
      ],
      "metadata": {
        "id": "83TVE4OiA6yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import scipy.linalg as lg\n",
        "import sys\n",
        "P=6\n",
        "gamma=0.7\n",
        "P\n",
        "T = 600\n",
        "T_enc = 300\n",
        "cen = (2 * (np.arange(1,P+1, dtype=float) )- 3) / (2 * (P - 2))\n",
        "#print(cen)\n",
        "tau=300\n",
        "marg=-(sys.maxsize)- 1\n",
        "wid = 1 / (gamma * (P - 2))\n",
        "#print(wid)\n",
        "n_sample, n_feature =train_sh.shape\n",
        "n_feature -= 1\n",
        "n_class = np.size(np.unique(train_sh.iloc[:, -1]))\n",
        "class_of_neuron=np.zeros(1500)\n",
        "n_neuron=0\n",
        "n_neurons_in_class=np.zeros(n_class) #class size change here\n",
        "weight_ik=np.empty((n_feature*P,1500))\n",
        "#initializing values for SEFRON\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "V_i=np.zeros((n_sample,n_feature*P),dtype=int)\n",
        "hid_thres=np.zeros(1500)\n",
        "T_input=T/3\n",
        "alpha=.60\n",
        "V_k=np.empty((n_sample,1500))\n",
        "\n",
        "delay_by_tau = np.arange(T, dtype=float) / 300\n",
        "norm_psp = np.triu(lg.toeplitz(delay_by_tau * np.exp(1 - delay_by_tau)))\n",
        "\n",
        "\n",
        "no_epochs=2 #no of epochs\n",
        "test_acc_hid=[]\n",
        "train_acc_hid=[]\n",
        "for e in range(no_epochs):\n",
        "  count_acc=0\n",
        "  CC_n_t=[]\n",
        "  MC_n_t=[]\n",
        "  count_tacc=0\n",
        "  t_cc=-1\n",
        "  t_mc=-1\n",
        "  #change the order of the input sample\n",
        "  #cnf_mat = np.zeros((3, 3))\n",
        "  for s in range(n_sample):\n",
        "    CC_n=[]\n",
        "    MC_n=[]\n",
        "\n",
        "    curr_class=train_sh.iloc[s,-1]\n",
        "\n",
        "    sample=train_sh.iloc[s,:-1]\n",
        "    sample = sample[sp.newaxis, :]\n",
        "\n",
        "    spike_pattern = pop_code(sample, cen[:, sp.newaxis], wid, P,gamma,T,T_enc)\n",
        "\n",
        "\n",
        "    if ((n_neurons_in_class[curr_class-1]==0)and (e==0)):\n",
        "\n",
        "      n_neurons_in_class[curr_class-1]+=1\n",
        "      (neuron,spike_time)=spike_pattern.nonzero()\n",
        "      class_of_neuron[n_neuron]=curr_class\n",
        "      psp4spike = norm_psp[spike_time, 200]\n",
        "      total_pre_psp = np.zeros(n_feature*P)\n",
        "      np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "      weight_ik[:, n_neuron] = total_pre_psp / np.sum(total_pre_psp)\n",
        "      hid_thres[n_neuron] = np.dot(weight_ik[:, n_neuron].T, total_pre_psp)\n",
        "      n_neuron += 1\n",
        "\n",
        "    else:\n",
        "      #computing hidden neuron output.\n",
        "      print(n_neuron)\n",
        "      hidden=hid_out(spike_pattern,weight_ik[:,:n_neuron],hid_thres[:n_neuron],norm_psp)\n",
        "\n",
        "      ## We know what time the neurons have spiked using the hidden variable above\n",
        "\n",
        "\n",
        "\n",
        "      #index of hidden neurons that have spiked\n",
        "      spiked_hid_neurons=np.argmax(hidden.toarray(),axis=1)\n",
        "\n",
        "\n",
        "      fired_neurons = np.squeeze(np.asarray(hidden[np.arange(n_neuron), spiked_hid_neurons] == 1))\n",
        "\n",
        "      #boolean of which neurons have fired\n",
        "      spiked_hid_neurons[np.logical_not(fired_neurons)] = T+1\n",
        "\n",
        "      for i in range(n_neuron):\n",
        "        if(class_of_neuron[i]==curr_class):\n",
        "\n",
        "          CC_n.append(i)\n",
        "\n",
        "      min=T+1\n",
        "      CC=-1\n",
        "      for i in CC_n:\n",
        "        if(spiked_hid_neurons[i]<min):\n",
        "          min=spiked_hid_neurons[i]\n",
        "          CC=i\n",
        "      t_cc=min\n",
        "      CC=i\n",
        "\n",
        "\n",
        "      for i in range(n_neuron):\n",
        "        if(class_of_neuron[i]!=curr_class):\n",
        "\n",
        "          MC_n.append(i)\n",
        "      min_2=T+1\n",
        "      MC=-1\n",
        "\n",
        "      for i in MC_n:\n",
        "        if(i<n_neuron):\n",
        "          if(spiked_hid_neurons[i]<min_2):\n",
        "            min_2=spiked_hid_neurons[i]\n",
        "            MC=i\n",
        "      t_mc=min_2\n",
        "      MC=i\n",
        "\n",
        "      #margin calculation\n",
        "\n",
        "      if((t_mc!=-1) and(t_cc!=-1)):\n",
        "        marg=t_mc-t_cc\n",
        "      T_m=28\n",
        "      T_a=alpha*T\n",
        "      eta=0.001\n",
        "      if (e==0):\n",
        "        if((CC==-1) or ((CC!=-1)and (t_cc>T_a))and (n_neuron < 1500)):\n",
        "          n_neurons_in_class[curr_class-1]+=1\n",
        "          class_of_neuron[n_neuron]=curr_class\n",
        "          spike_pattern = pop_code(sample, cen[:, sp.newaxis], wid, P,gamma,T,T_enc)\n",
        "          (neuron,spike_time)=spike_pattern.nonzero()\n",
        "\n",
        "          psp4spike = norm_psp[spike_time, 200]\n",
        "          total_pre_psp = np.zeros(n_feature * P)\n",
        "          np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "\n",
        "                  # initialize the weights and threshold\n",
        "          weight_ik[:, n_neuron] = total_pre_psp / np.sum(total_pre_psp)\n",
        "          hid_thres[n_neuron] = np.dot(weight_ik[:, n_neuron].T, total_pre_psp)\n",
        "          n_neuron+=1\n",
        "\n",
        "        elif((MC!=-1)and(marg<T_m)):\n",
        "          (neuron, spike_time) = spike_pattern.nonzero()\n",
        "\n",
        "          if(t_cc<601):\n",
        "            psp4spike = norm_psp[spike_time, t_cc]\n",
        "            total_pre_psp = np.zeros(n_feature * P)\n",
        "            np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "            weight_ik[:, CC] += eta * sp.maximum((total_pre_psp / np.sum(total_pre_psp)) - weight_ik[:, CC], 0)\n",
        "\n",
        "\n",
        "          psp4spike = norm_psp[spike_time, t_mc]\n",
        "          total_pre_psp = np.zeros(n_feature * P)\n",
        "          np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "          weight_ik[:, MC] -= eta * sp.maximum((total_pre_psp / np.sum(total_pre_psp)) - weight_ik[:, MC], 0)\n",
        "\n",
        "      elif(e!=0):\n",
        "        if((MC!=-1)and(marg<T_m)):\n",
        "          (neuron, spike_time) = spike_pattern.nonzero()\n",
        "\n",
        "          if(t_cc<601):\n",
        "            psp4spike = norm_psp[spike_time, t_cc]\n",
        "            total_pre_psp = np.zeros(n_feature * P)\n",
        "            np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "            weight_ik[:, CC] += eta * sp.maximum((total_pre_psp / np.sum(total_pre_psp)) - weight_ik[:, CC], 0)\n",
        "            psp4spike = norm_psp[spike_time, t_mc]\n",
        "            total_pre_psp = np.zeros(n_feature * P)\n",
        "            np.add.at(total_pre_psp, neuron, psp4spike)\n",
        "            weight_ik[:, MC] -= eta * sp.maximum((total_pre_psp / np.sum(total_pre_psp)) - weight_ik[:, MC], 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if(t_cc<t_mc):\n",
        "      count_acc+=1\n",
        "     #change the s==79 value to the number of rows in training file minus 1\n",
        "    if(s==65):\n",
        "      acc=count_acc/(s+1)\n",
        "      train_acc_hid.append(acc)\n",
        "\n",
        "\n",
        "  test_sample,test_feat=test_sh.shape\n",
        "  #count_acc=0\n",
        "  for t in range(test_sample):\n",
        "    curr_class_test=test_sh.iloc[t,-1]\n",
        "\n",
        "    sample_test=test_sh.iloc[t,:-1]\n",
        "    sample_test = sample_test[sp.newaxis, :]\n",
        "\n",
        "    spike_pattern_test = pop_code(sample_test, cen[:, sp.newaxis], wid, P,gamma,T,T_enc)\n",
        "    hidden_test=hid_out(spike_pattern_test,weight_ik[:,:n_neuron],hid_thres[:n_neuron],norm_psp)\n",
        "\n",
        "      ## We know what time the neurons have spiked using the hidden variable above\n",
        "\n",
        "\n",
        "\n",
        "      #index of hidden neurons that have spiked\n",
        "    spiked_hid_neurons_test=np.argmax(hidden_test.toarray(),axis=1)\n",
        "\n",
        "\n",
        "    fired_neurons_test = np.squeeze(np.asarray(hidden_test[np.arange(n_neuron), spiked_hid_neurons_test] == 1))\n",
        "\n",
        "    #boolean of which neurons have fired\n",
        "    spiked_hid_neurons_test[np.logical_not(fired_neurons_test)] = T+1\n",
        "\n",
        "    for i in range(n_neuron):\n",
        "      if(class_of_neuron[i]==curr_class_test):\n",
        "\n",
        "        CC_n_t.append(i)\n",
        "\n",
        "    min_t=T+1\n",
        "    CC_t=-1\n",
        "    for i in CC_n_t:\n",
        "      if(spiked_hid_neurons_test[i]<min_t):\n",
        "        min_t=spiked_hid_neurons_test[i]\n",
        "        CC_t=i\n",
        "    t_tcc=min_t\n",
        "    CC_t=i\n",
        "\n",
        "\n",
        "    for i in range(n_neuron):\n",
        "      if(class_of_neuron[i]!=curr_class_test):\n",
        "\n",
        "        MC_n_t.append(i)\n",
        "    min_2_t=T+1\n",
        "    MC_t=-1\n",
        "\n",
        "\n",
        "    for i in MC_n_t:\n",
        "      if(i<n_neuron):\n",
        "        if(spiked_hid_neurons_test[i]<min_2_t):\n",
        "          min_2_t=spiked_hid_neurons_test[i]\n",
        "          MC_t=i\n",
        "    t_tmc=min_2_t\n",
        "    MC_t=i\n",
        "    if(t_tcc<t_tmc):\n",
        "      count_acc+=1\n",
        "    #change the t==10 value to the number of rows in testing file minus 1\n",
        "    if(t==64):\n",
        "      acc=count_acc/(t+1)\n",
        "      test_acc_hid.append(acc)\n",
        "\n",
        "  '''for i in range(n_neuron):\n",
        "    print(class_of_neuron[i])'''"
      ],
      "metadata": {
        "id": "OylfPv5sWLT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4232e156-8f4a-4c6b-e882-ceb49f2a6702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "True 1\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n",
            "4\n",
            "[ True  True  True  True] 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8ta0djHWfgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}